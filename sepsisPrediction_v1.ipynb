{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial steps\n",
    "1. File -> New -> Terminal\n",
    "2. In the terminal, type following commands\n",
    "        mkdir med264/\n",
    "        cd med264/\n",
    "        wget https://archive.physionet.org/users/shared/challenge-2019/training_setB.zip\n",
    "        unzip training_setB.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for loading physionet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a\n",
    "def load_challenge_data(file):\n",
    "    with open(file, 'r') as f:\n",
    "        header = f.readline().strip()\n",
    "        column_names = header.split('|')\n",
    "        data = np.loadtxt(f, delimiter='|')\n",
    "\n",
    "    # Ignore SepsisLabel column if present.\n",
    "    if column_names[-1] == 'SepsisLabel':\n",
    "        column_names = column_names[:-1]\n",
    "        labels = data[:, -1]\n",
    "        data = data[:, :-1]\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "1. Load the list of files in the input directory\n",
    "2. Split the records to training and testing set\n",
    "3. Convert the data into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the input directory\n",
    "input_directory = 'training_setB'\n",
    "\n",
    "# Find files.\n",
    "files = []\n",
    "for f in os.listdir(input_directory):\n",
    "    if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('psv'):\n",
    "        files.append(f)\n",
    "\n",
    "# Total number of files in the input directory\n",
    "num_files = len(files)\n",
    "\n",
    "# SPlit the files into training and testing set\n",
    "# 80% of data -> Training\n",
    "# 20% of data -> Testing\n",
    "shuffled_indices = np.random.permutation(num_files)\n",
    "train_indices = shuffled_indices[0:int(np.round(0.8*num_files))]\n",
    "test_indices = shuffled_indices[int(np.round(0.8*num_files)):]\n",
    "\n",
    "train_files = [] # List of files containing training data\n",
    "test_files = [] # List of files containing testing data\n",
    "for i in range(train_indices.shape[0]):\n",
    "    train_files.append(files[train_indices[i]])\n",
    "    \n",
    "for i in range(test_indices.shape[0]):\n",
    "    test_files.append(files[test_indices[i]])\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []\n",
    "    \n",
    "for i, f in enumerate(train_files):\n",
    "    # Load data.\n",
    "    input_file = os.path.join(input_directory, f)\n",
    "    data_patient, label_patient = load_challenge_data(input_file)\n",
    "    train_data.append(data_patient)\n",
    "    train_labels.append(label_patient)\n",
    "\n",
    "train_data = np.concatenate(train_data)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "    \n",
    "for i, f in enumerate(test_files):\n",
    "    # Load data.\n",
    "    input_file = os.path.join(input_directory, f)\n",
    "    data_patient, label_patient = load_challenge_data(input_file)\n",
    "    test_data.append(data_patient)\n",
    "    test_labels.append(label_patient)\n",
    "\n",
    "test_data = np.concatenate(test_data)\n",
    "test_labels = np.concatenate(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization of data\n",
    "1. Compute the mean and standard deviation of ONLY training set\n",
    "2. Use the statistics computed to standardize Training and Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = np.nanmean(train_data, axis = 0)\n",
    "x_std = np.nanstd(train_data, axis = 0)\n",
    "\n",
    "# For Nan entries, replace with 0\n",
    "# For the remaining entries, standardize with mean and std\n",
    "train_data = np.nan_to_num((train_data - x_mean) / x_std)\n",
    "test_data = np.nan_to_num((test_data - x_mean) / x_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "1. We will use a simple Multinomial logistic regression model to model the data\n",
    "\n",
    "### Exercise 1\n",
    "1. Use a different penalty for Logistic Regression (LR) and plot the AUC curves\n",
    "2. Use a Support Vector Machine classifier instead of LR and plot the AUC curves\n",
    "        https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "3. Use a Random Forest Classifier instead of LR and plot the AUC curves \n",
    "        https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(penalty = 'l2', random_state=0, solver='lbfgs', max_iter = 150,\n",
    "                           multi_class='multinomial').fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction\n",
    "1. Use the trained model to get output probability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_score = model.predict_proba(train_data)\n",
    "test_pred_score = model.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "1. Compute Area Under the Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Compute AUC of training set\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n",
    "fpr_train, tpr_train, _ = metrics.roc_curve(train_labels, train_pred_score[:,1], pos_label = 1)\n",
    "train_auc = metrics.auc(fpr_train, tpr_train)\n",
    "\n",
    "# Compute AUC of testing set\n",
    "fpr_test, tpr_test, _ = metrics.roc_curve(test_labels, test_pred_score[:,1], pos_label = 1)\n",
    "test_auc = metrics.auc(fpr_test, tpr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot AUC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training AUC\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr_train, tpr_train, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % train_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for training data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing AUC\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr_test, tpr_test, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % test_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for testing data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Interpretability\n",
    "\n",
    "Methods such as LIME [1], SHAP [2] allow for revealing the top features contributing to the predicted score at a local level. \n",
    "\n",
    "The python library SHAP (https://shap.readthedocs.io/en/latest/) uses the method of shapley values to determine the top contributing features.\n",
    "\n",
    "Use the python library SHAP and show the force plot, dependance plot and summary plot for each of the models developed above.\n",
    "Some examples using SHAP: https://shap.readthedocs.io/en/latest/examples.html\n",
    "\n",
    "[1] https://christophm.github.io/interpretable-ml-book/lime.html\n",
    "\n",
    "[2] https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
